[
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-4.html",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-4.html",
    "title": "Hands-on exercise 05-4: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "pacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\n\nwh &lt;- read_csv(\"WHData-2018.csv\")\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-4.html#getting-started",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-4.html#getting-started",
    "title": "Hands-on exercise 05-4: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "pacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\n\nwh &lt;- read_csv(\"WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-4.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-4.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on exercise 05-4: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "ggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-4.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-4.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on exercise 05-4: Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "wh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications homepage.\n\n\n\n\n\nIn this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html",
    "title": "Hands-on exercise 05-2: Visual Correlation Analysis",
    "section": "",
    "text": "pacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\n\nwine &lt;- read_csv(\"wine_quality.csv\")\n\n\n\n\n\n\n\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, we can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram. ggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: , , , , , , , , , .corrmethodp.matsig.levelggthemecolorslabpchlegend.titledigits The sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\nto build a facet plot, the only argument needed is .grouping.var Behind group_ggcorrmat(), patchwork package is used to create the multiplot. argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.plotgrid.args Likewise, argument is calling plot annotation arguments of patchwork package.annotation.args\n\n\n\n\n\n\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html#getting-started",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html#getting-started",
    "title": "Hands-on exercise 05-2: Visual Correlation Analysis",
    "section": "",
    "text": "pacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\n\nwine &lt;- read_csv(\"wine_quality.csv\")"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on exercise 05-2: Visual Correlation Analysis",
    "section": "",
    "text": "pairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, we can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on exercise 05-2: Visual Correlation Analysis",
    "section": "",
    "text": "ggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram. ggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: , , , , , , , , , .corrmethodp.matsig.levelggthemecolorslabpchlegend.titledigits The sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\nto build a facet plot, the only argument needed is .grouping.var Behind group_ggcorrmat(), patchwork package is used to create the multiplot. argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.plotgrid.args Likewise, argument is calling plot annotation arguments of patchwork package.annotation.args"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-2.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on exercise 05-2: Visual Correlation Analysis",
    "section": "",
    "text": "Before we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Take-home exercise/Take-home exercise 1.html",
    "href": "Take-home exercise/Take-home exercise 1.html",
    "title": "Take-home exercise 1",
    "section": "",
    "text": "With the rising prevalence of cardiovascular diseases worldwide, public attention to these conditions has also increased. Therefore, using data to identify risk factors for heart disease is crucial. This dataset focuses on Japanese citizens, recording their health status, risk factors, and personal conditions to explore potential insights within the data.\n\n\n\nThis take-home exercise aims to apply appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to analyze the data. Based on the analysis and observations, it also briefly suggests potential insights for future research to help policymakers improve public healthcare.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nfunction\n\n\n\n\nggplot2\nFor data visualization, creating high-quality charts.\n\n\ndplyr\nFor data manipulation, providing efficient filtering, transformation, and summarization functions.\n\n\nscales\nFor adjusting scales, axes, and formatting in ggplot2 charts.\n\n\ntidyverse\nA collection of data science packages, for data processing and visualization.\n\n\nggridges\nFor creating ridge plots, useful for visualizing density distributions.\n\n\ncolorspace\nFor advanced color management, including color transformations and palette selection.\n\n\nplotly\nFor creating interactive charts, including 3D and dynamic visualizations.\n\n\ncrosstalk\nFor enabling interactive data sharing between different HTML widgets (e.g., plotly, DT tables).\n\n\nDT\nFor creating interactive HTML data tables.\n\n\nggdist\nFor statistical visualizations, such as distributions and uncertainty representations.\n\n\ngganimate\nFor animating ggplot2 charts, enabling time-series or dynamic visualizations.\n\n\n\n\npacman::p_load(ggplot2, dplyr, scales, tidyverse, ggridges, colorspace, plotly, crosstalk, DT, ggdist, gganimate) \n\n\n\n\nThe dataset comes from Kaggle, with 1 table, 32 columns, and 30,000 rows. Each row of data represents the basic physical condition of a Japanese citizen.\n\n\n\n\nheart_attack_data &lt;- read.csv(\"japan_heart_attack_dataset.csv\")\n\n\n\n\nCheck the data set for duplicate data to prevent interference with subsequent analysis.\n\nheart_attack_data[duplicated(heart_attack_data), ]\n\n [1] Age                     Gender                  Region                 \n [4] Smoking_History         Diabetes_History        Hypertension_History   \n [7] Cholesterol_Level       Physical_Activity       Diet_Quality           \n[10] Alcohol_Consumption     Stress_Levels           BMI                    \n[13] Heart_Rate              Systolic_BP             Diastolic_BP           \n[16] Family_History          Heart_Attack_Occurrence Extra_Column_1         \n[19] Extra_Column_2          Extra_Column_3          Extra_Column_4         \n[22] Extra_Column_5          Extra_Column_6          Extra_Column_7         \n[25] Extra_Column_8          Extra_Column_9          Extra_Column_10        \n[28] Extra_Column_11         Extra_Column_12         Extra_Column_13        \n[31] Extra_Column_14         Extra_Column_15        \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nThe result shows no duplicate data.\n\n\n\nAccording to Fabian Sanchis-Gomar. (2016). Epidemiology of coronary heart disease and acute coronary syndrome. This analysis will focus on the following factors:\n\n\n\n\n\n\n\n\n\nNo\nName\nDescription\nType of Variable\n\n\n\n\n1\nAge\nAge of respondents\nNumerical\n\n\n2\nGender\nMale or Female\nCategorical\n\n\n3\nSmoking History\nYes or No\nCategorical\n\n\n4\nDiabetes History\nYes or No\nCategorical\n\n\n5\nPhysical Activity\nIntensity level of physical activity\nNumerical\n\n\n6\nCholesterol_Level\nCholesterol levels of respondents\nNumerical\n\n\n7\nStress Level\nStress Level of respondents\nNumerical\n\n\n8\nBMI\nWeight / Height ², used to determine the weight health status of respondents\nNumerical\n\n\n9\nFamily History\nYes or No\nCategorical\n\n\n10\nHeart Attack Occurrence\nYes or No\nCategorical\n\n\n\nTherefore, only relevant columns are selected for analysis.\n\nselect_data &lt;- heart_attack_data %&gt;% select (Age, Gender, Smoking_History, Diabetes_History, Physical_Activity, Cholesterol_Level, Stress_Levels, BMI, Family_History, Heart_Attack_Occurrence )\n\nUse the code to perform checks on the newly generated dataset.\n\nglimpse(select_data)\n\nRows: 30,000\nColumns: 10\n$ Age                     &lt;int&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n\n\n\n\n\n\nany(is.na(select_data))\n\n[1] FALSE\n\n\nThe result shows no missing values.\n\n\n\n\n\n\nSince the number of respondents in each age group is not necessarily the same (i.e., the age distribution is not necessarily balanced), it is also necessary to consider the age distribution of the respondents.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(select_data, aes(x = Age)) +\n  geom_histogram(data = select_data, aes(x = Age), binwidth = 5, fill = \"lightgray\", color = \"black\", alpha = 0.5) +\n  geom_histogram(data = subset(select_data, Heart_Attack_Occurrence == \"Yes\"), aes(x = Age), binwidth = 5, fill = \"tomato\", color = \"black\", alpha = 0.7) +\n  labs(x = \"Age\", y = \"Count\", title = \"Age Distribution for All People and Heart Attack Patients\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInsight 1\n\n\n\n\nAs we can see from the graph above, there is no significant relationship between whether or not a person has heart disease and their age in Japan. The distribution of the number of people with heart disease does not change significantly with age, and the number of people with heart disease is basically the same as the age distribution of the respondents.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(scales)\ndata_summary &lt;- select_data %&gt;%\n  group_by(Heart_Attack_Occurrence, Gender) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(total_count = sum(count), percentage = count / total_count)\n\nggplot(data_summary, aes(x = \"\", y = count, fill = interaction(Heart_Attack_Occurrence, Gender))) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\") +  \n  labs(title = \"Heart Attack Occurrence vs Gender\", fill = \"Heart Attack Occurrence and Gender\") +\n  theme_void() +  \n  scale_fill_manual(values = c(\"lightcoral\", \"salmon\", \"lightsteelblue\", \"skyblue\")) +\n  geom_text(aes(label = percent(percentage)), position = position_stack(vjust = 0.5), color = \"black\")\n\n\n\n\n\n\n\n\n\n\nInsight 2\n\n\n\n\nFrom the above figure, we can see that the proportion of men and women with heart disease is almost the same when the ratio of men and women respondents is almost the same, so it can be seen that whether or not to suffer from heart disease has nothing to do with the gender of the patients in Japan.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_prop &lt;- select_data %&gt;%\n  pivot_longer(cols = c(Diabetes_History, Smoking_History, Family_History),\n               names_to = \"Risk_Factor\",\n               values_to = \"Status\") %&gt;%\n  group_by(Risk_Factor, Status) %&gt;%\n  summarise(Heart_Attack_Rate = mean(Heart_Attack_Occurrence == \"Yes\")) %&gt;%\n  ungroup()\n\nggplot(df_prop, aes(x = Status, y = Heart_Attack_Rate, fill = Status)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ Risk_Factor) +\n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence by Risk Factors\",\n       x = \"Risk Factor Status (Yes/No)\",\n       y = \"Heart Attack Occurrence Rate\",\n       fill = \"Risk Factor Status\") +\n  scale_y_continuous(labels = scales::percent) +  # Convert Y-axis to percentage\n  scale_fill_manual(values = c(\"No\" = \"skyblue\", \"Yes\" = \"lightcoral\"))\n\n\n\n\n\n\n\n\n\n\nInsight 3\n\n\n\n\nAs we can see from the graph above, people with diabetes and a history of smoking are a bit more likely to get heart disease than the general population in Japan, but the overall percentage difference is not significant..\n\n\n\n\n\n\nBecause the number of people at different exercise intensities is itself different, the ratio of their numbers will also be considered.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(select_data, aes(x = Physical_Activity)) +\n  geom_bar(data = select_data, aes(x = Physical_Activity), binwidth = 5, fill = \"lightgray\", color = \"black\", alpha = 0.5) +\n  geom_bar(data = subset(select_data, Heart_Attack_Occurrence == \"Yes\"), aes(x = Physical_Activity), binwidth = 5, fill = \"tomato\", color = \"black\", alpha = 0.7) +\n  labs(x = \"Age\", y = \"Count\", title = \"Age Distribution for All People and Heart Attack Patients\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInsight 4\n\n\n\n\nAs we can see from the figure above, the proportion of people with the disease in each group of different exercise intensities is almost the same as the proportion of the overall number of people in their group, so there is no significant relationship between the two either.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=select_data, \n       aes(x = Cholesterol_Level, \n           colour = Heart_Attack_Occurrence)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n\nInsight 5\n\n\n\n\nLooking at the cholesterol density graph of the respondents above, there is no significant difference between those at risk for heart disease and the healthy population. It can be seen that cholesterol levels are not a major potential cause of heart disease in Japan.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(select_data, \n       aes(x = BMI, \n           y = Heart_Attack_Occurrence)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"BMI VS Heart Attack\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nInsight 6\n\n\n\n\nThe BMI ridgeline models for the general population and the diseased population are nearly identical, and ostensibly their differences are not significant enough for a significant relationship to exist.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nselect_data %&gt;%\n  ggplot(aes(x = Heart_Attack_Occurrence, \n             y = Stress_Levels)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean stress level\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nInsight 7\n\n\n\n\nThe confidence intervals for the stress levels of the diseased and normal populations are nearly the same, and ostensibly the differences were not significant.\n\n\n\n\n\n\n\nAccording to existing studies, in general, factors such as age, blood glucose, blood pressure, etc., can be potential factors for developing heart disease, but the present dataset does not present any variables that are clearly associated with heart disease, which is a very strange matter. Based on the above results, I make the following speculation:\n\nThe form of the dataset may be not suitable for analyzing heart disease\nThe Japanese have a low incidence of cardiovascular disease worldwide, and their healthy lifestyle may make the role of these potentially heart disease-causing variables less pronounced\nGiven the complexity of the causative factors of heart disease, it may not be possible to analyze the relationship of only one pair of variables to derive valid insights, so subsequent logistic regression models or decision trees may be used to further analyze the dataset and incorporate additional medical information to determine the weight of these potential causative factors.\n\n\n\n\nhttps://atm.amegroups.org/article/view/10896/11530\nhttps://www.kaggle.com/datasets/ashaychoudhary/heart-attack-in-japan-youth-vs-adult"
  },
  {
    "objectID": "Take-home exercise/Take-home exercise 1.html#introduction",
    "href": "Take-home exercise/Take-home exercise 1.html#introduction",
    "title": "Take-home exercise 1",
    "section": "",
    "text": "With the rising prevalence of cardiovascular diseases worldwide, public attention to these conditions has also increased. Therefore, using data to identify risk factors for heart disease is crucial. This dataset focuses on Japanese citizens, recording their health status, risk factors, and personal conditions to explore potential insights within the data.\n\n\n\nThis take-home exercise aims to apply appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to analyze the data. Based on the analysis and observations, it also briefly suggests potential insights for future research to help policymakers improve public healthcare."
  },
  {
    "objectID": "Take-home exercise/Take-home exercise 1.html#data-prepare",
    "href": "Take-home exercise/Take-home exercise 1.html#data-prepare",
    "title": "Take-home exercise 1",
    "section": "",
    "text": "Package\nfunction\n\n\n\n\nggplot2\nFor data visualization, creating high-quality charts.\n\n\ndplyr\nFor data manipulation, providing efficient filtering, transformation, and summarization functions.\n\n\nscales\nFor adjusting scales, axes, and formatting in ggplot2 charts.\n\n\ntidyverse\nA collection of data science packages, for data processing and visualization.\n\n\nggridges\nFor creating ridge plots, useful for visualizing density distributions.\n\n\ncolorspace\nFor advanced color management, including color transformations and palette selection.\n\n\nplotly\nFor creating interactive charts, including 3D and dynamic visualizations.\n\n\ncrosstalk\nFor enabling interactive data sharing between different HTML widgets (e.g., plotly, DT tables).\n\n\nDT\nFor creating interactive HTML data tables.\n\n\nggdist\nFor statistical visualizations, such as distributions and uncertainty representations.\n\n\ngganimate\nFor animating ggplot2 charts, enabling time-series or dynamic visualizations.\n\n\n\n\npacman::p_load(ggplot2, dplyr, scales, tidyverse, ggridges, colorspace, plotly, crosstalk, DT, ggdist, gganimate) \n\n\n\n\nThe dataset comes from Kaggle, with 1 table, 32 columns, and 30,000 rows. Each row of data represents the basic physical condition of a Japanese citizen.\n\n\n\n\nheart_attack_data &lt;- read.csv(\"japan_heart_attack_dataset.csv\")\n\n\n\n\nCheck the data set for duplicate data to prevent interference with subsequent analysis.\n\nheart_attack_data[duplicated(heart_attack_data), ]\n\n [1] Age                     Gender                  Region                 \n [4] Smoking_History         Diabetes_History        Hypertension_History   \n [7] Cholesterol_Level       Physical_Activity       Diet_Quality           \n[10] Alcohol_Consumption     Stress_Levels           BMI                    \n[13] Heart_Rate              Systolic_BP             Diastolic_BP           \n[16] Family_History          Heart_Attack_Occurrence Extra_Column_1         \n[19] Extra_Column_2          Extra_Column_3          Extra_Column_4         \n[22] Extra_Column_5          Extra_Column_6          Extra_Column_7         \n[25] Extra_Column_8          Extra_Column_9          Extra_Column_10        \n[28] Extra_Column_11         Extra_Column_12         Extra_Column_13        \n[31] Extra_Column_14         Extra_Column_15        \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nThe result shows no duplicate data.\n\n\n\nAccording to Fabian Sanchis-Gomar. (2016). Epidemiology of coronary heart disease and acute coronary syndrome. This analysis will focus on the following factors:\n\n\n\n\n\n\n\n\n\nNo\nName\nDescription\nType of Variable\n\n\n\n\n1\nAge\nAge of respondents\nNumerical\n\n\n2\nGender\nMale or Female\nCategorical\n\n\n3\nSmoking History\nYes or No\nCategorical\n\n\n4\nDiabetes History\nYes or No\nCategorical\n\n\n5\nPhysical Activity\nIntensity level of physical activity\nNumerical\n\n\n6\nCholesterol_Level\nCholesterol levels of respondents\nNumerical\n\n\n7\nStress Level\nStress Level of respondents\nNumerical\n\n\n8\nBMI\nWeight / Height ², used to determine the weight health status of respondents\nNumerical\n\n\n9\nFamily History\nYes or No\nCategorical\n\n\n10\nHeart Attack Occurrence\nYes or No\nCategorical\n\n\n\nTherefore, only relevant columns are selected for analysis.\n\nselect_data &lt;- heart_attack_data %&gt;% select (Age, Gender, Smoking_History, Diabetes_History, Physical_Activity, Cholesterol_Level, Stress_Levels, BMI, Family_History, Heart_Attack_Occurrence )\n\nUse the code to perform checks on the newly generated dataset.\n\nglimpse(select_data)\n\nRows: 30,000\nColumns: 10\n$ Age                     &lt;int&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n\n\n\n\n\n\nany(is.na(select_data))\n\n[1] FALSE\n\n\nThe result shows no missing values."
  },
  {
    "objectID": "Take-home exercise/Take-home exercise 1.html#insights",
    "href": "Take-home exercise/Take-home exercise 1.html#insights",
    "title": "Take-home exercise 1",
    "section": "",
    "text": "Since the number of respondents in each age group is not necessarily the same (i.e., the age distribution is not necessarily balanced), it is also necessary to consider the age distribution of the respondents.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(select_data, aes(x = Age)) +\n  geom_histogram(data = select_data, aes(x = Age), binwidth = 5, fill = \"lightgray\", color = \"black\", alpha = 0.5) +\n  geom_histogram(data = subset(select_data, Heart_Attack_Occurrence == \"Yes\"), aes(x = Age), binwidth = 5, fill = \"tomato\", color = \"black\", alpha = 0.7) +\n  labs(x = \"Age\", y = \"Count\", title = \"Age Distribution for All People and Heart Attack Patients\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInsight 1\n\n\n\n\nAs we can see from the graph above, there is no significant relationship between whether or not a person has heart disease and their age in Japan. The distribution of the number of people with heart disease does not change significantly with age, and the number of people with heart disease is basically the same as the age distribution of the respondents.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(scales)\ndata_summary &lt;- select_data %&gt;%\n  group_by(Heart_Attack_Occurrence, Gender) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(total_count = sum(count), percentage = count / total_count)\n\nggplot(data_summary, aes(x = \"\", y = count, fill = interaction(Heart_Attack_Occurrence, Gender))) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\") +  \n  labs(title = \"Heart Attack Occurrence vs Gender\", fill = \"Heart Attack Occurrence and Gender\") +\n  theme_void() +  \n  scale_fill_manual(values = c(\"lightcoral\", \"salmon\", \"lightsteelblue\", \"skyblue\")) +\n  geom_text(aes(label = percent(percentage)), position = position_stack(vjust = 0.5), color = \"black\")\n\n\n\n\n\n\n\n\n\n\nInsight 2\n\n\n\n\nFrom the above figure, we can see that the proportion of men and women with heart disease is almost the same when the ratio of men and women respondents is almost the same, so it can be seen that whether or not to suffer from heart disease has nothing to do with the gender of the patients in Japan.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_prop &lt;- select_data %&gt;%\n  pivot_longer(cols = c(Diabetes_History, Smoking_History, Family_History),\n               names_to = \"Risk_Factor\",\n               values_to = \"Status\") %&gt;%\n  group_by(Risk_Factor, Status) %&gt;%\n  summarise(Heart_Attack_Rate = mean(Heart_Attack_Occurrence == \"Yes\")) %&gt;%\n  ungroup()\n\nggplot(df_prop, aes(x = Status, y = Heart_Attack_Rate, fill = Status)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ Risk_Factor) +\n  theme_minimal() +\n  labs(title = \"Heart Attack Occurrence by Risk Factors\",\n       x = \"Risk Factor Status (Yes/No)\",\n       y = \"Heart Attack Occurrence Rate\",\n       fill = \"Risk Factor Status\") +\n  scale_y_continuous(labels = scales::percent) +  # Convert Y-axis to percentage\n  scale_fill_manual(values = c(\"No\" = \"skyblue\", \"Yes\" = \"lightcoral\"))\n\n\n\n\n\n\n\n\n\n\nInsight 3\n\n\n\n\nAs we can see from the graph above, people with diabetes and a history of smoking are a bit more likely to get heart disease than the general population in Japan, but the overall percentage difference is not significant..\n\n\n\n\n\n\nBecause the number of people at different exercise intensities is itself different, the ratio of their numbers will also be considered.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(select_data, aes(x = Physical_Activity)) +\n  geom_bar(data = select_data, aes(x = Physical_Activity), binwidth = 5, fill = \"lightgray\", color = \"black\", alpha = 0.5) +\n  geom_bar(data = subset(select_data, Heart_Attack_Occurrence == \"Yes\"), aes(x = Physical_Activity), binwidth = 5, fill = \"tomato\", color = \"black\", alpha = 0.7) +\n  labs(x = \"Age\", y = \"Count\", title = \"Age Distribution for All People and Heart Attack Patients\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInsight 4\n\n\n\n\nAs we can see from the figure above, the proportion of people with the disease in each group of different exercise intensities is almost the same as the proportion of the overall number of people in their group, so there is no significant relationship between the two either.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=select_data, \n       aes(x = Cholesterol_Level, \n           colour = Heart_Attack_Occurrence)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n\nInsight 5\n\n\n\n\nLooking at the cholesterol density graph of the respondents above, there is no significant difference between those at risk for heart disease and the healthy population. It can be seen that cholesterol levels are not a major potential cause of heart disease in Japan.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(select_data, \n       aes(x = BMI, \n           y = Heart_Attack_Occurrence)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"BMI VS Heart Attack\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nInsight 6\n\n\n\n\nThe BMI ridgeline models for the general population and the diseased population are nearly identical, and ostensibly their differences are not significant enough for a significant relationship to exist.\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nselect_data %&gt;%\n  ggplot(aes(x = Heart_Attack_Occurrence, \n             y = Stress_Levels)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean stress level\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nInsight 7\n\n\n\n\nThe confidence intervals for the stress levels of the diseased and normal populations are nearly the same, and ostensibly the differences were not significant."
  },
  {
    "objectID": "Take-home exercise/Take-home exercise 1.html#summary-and-conclusion",
    "href": "Take-home exercise/Take-home exercise 1.html#summary-and-conclusion",
    "title": "Take-home exercise 1",
    "section": "",
    "text": "According to existing studies, in general, factors such as age, blood glucose, blood pressure, etc., can be potential factors for developing heart disease, but the present dataset does not present any variables that are clearly associated with heart disease, which is a very strange matter. Based on the above results, I make the following speculation:\n\nThe form of the dataset may be not suitable for analyzing heart disease\nThe Japanese have a low incidence of cardiovascular disease worldwide, and their healthy lifestyle may make the role of these potentially heart disease-causing variables less pronounced\nGiven the complexity of the causative factors of heart disease, it may not be possible to analyze the relationship of only one pair of variables to derive valid insights, so subsequent logistic regression models or decision trees may be used to further analyze the dataset and incorporate additional medical information to determine the weight of these potential causative factors."
  },
  {
    "objectID": "Take-home exercise/Take-home exercise 1.html#references",
    "href": "Take-home exercise/Take-home exercise 1.html#references",
    "title": "Take-home exercise 1",
    "section": "",
    "text": "https://atm.amegroups.org/article/view/10896/11530\nhttps://www.kaggle.com/datasets/ashaychoudhary/heart-attack-in-japan-youth-vs-adult"
  },
  {
    "objectID": "In-class exercise/In-class exercise 03.html",
    "href": "In-class exercise/In-class exercise 03.html",
    "title": "In-class exercise 03",
    "section": "",
    "text": "Tableau Public link: https://public.tableau.com/app/profile/haoyang.xie/vizzes"
  },
  {
    "objectID": "In-class exercise/In-class exercise 01.html",
    "href": "In-class exercise/In-class exercise 01.html",
    "title": "In-class exercise 01",
    "section": "",
    "text": "Tableau Public link: https://public.tableau.com/app/profile/haoyang.xie/vizzes"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html",
    "title": "Hands-on exercise 05-5: Treemap Visualisation with R",
    "section": "",
    "text": "pacman::p_load(treemap, treemapify, tidyverse) \n\n\n\n\n\n\n\n\nrealis2018 &lt;- read_csv(\"realis2018.csv\")\n\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and compute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively. Two key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained. grouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables. mutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”). sample_n() and sample_frac() sample the specified number/fraction of rows in each group. summarise() computes the summary for each group. In our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative To overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html#getting-started",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html#getting-started",
    "title": "Hands-on exercise 05-5: Treemap Visualisation with R",
    "section": "",
    "text": "pacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html#data-wrangling",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html#data-wrangling",
    "title": "Hands-on exercise 05-5: Treemap Visualisation with R",
    "section": "",
    "text": "realis2018 &lt;- read_csv(\"realis2018.csv\")\n\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and compute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively. Two key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained. grouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables. mutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”). sample_n() and sample_frac() sample the specified number/fraction of rows in each group. summarise() computes the summary for each group. In our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html#designing-treemap-with-treemap-package",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html#designing-treemap-with-treemap-package",
    "title": "Hands-on exercise 05-5: Treemap Visualisation with R",
    "section": "",
    "text": "treemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative To overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-5.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on exercise 05-5: Treemap Visualisation with R",
    "section": "",
    "text": "ggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "In-class exercise/In-class exercise 02.html",
    "href": "In-class exercise/In-class exercise 02.html",
    "title": "In-class exercise 02",
    "section": "",
    "text": "Tableau Public link: https://public.tableau.com/app/profile/haoyang.xie/vizzes"
  },
  {
    "objectID": "In-class exercise/In-class exercise 04.html",
    "href": "In-class exercise/In-class exercise 04.html",
    "title": "In-class exercise 04",
    "section": "",
    "text": "Getting Started\nInstalling and loading R Packages\n\npacman:: p_load(tidyverse)\n\nImporting data into R environment\n\nexam_data &lt;- read.csv(\"Exam_data.csv\")\n\nPlotting\n\nggplot(data = exam_data,\n       aes(x = ENGLISH,\n           y = CLASS)) +\n  geom_boxplot()"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html",
    "title": "Hands-on exercise 05-3: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "pacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\n\nwh &lt;- read_csv(\"WHData-2018.csv\")\n\n\n\n\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format.\n\n\n\n\n\n\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nNote:\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nNote:\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\n\n\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html#getting-started",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html#getting-started",
    "title": "Hands-on exercise 05-3: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "pacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\n\nwh &lt;- read_csv(\"WHData-2018.csv\")\n\n\n\n\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html#static-heatmap",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html#static-heatmap",
    "title": "Hands-on exercise 05-3: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "wh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nNote:\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nNote:\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html#creating-interactive-heatmap",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html#creating-interactive-heatmap",
    "title": "Hands-on exercise 05-3: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "heatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north."
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html#data-trasformation",
    "href": "Hands-on exercise/Hands-on exercise 05/Hands-on exercise 05-3.html#data-trasformation",
    "title": "Hands-on exercise 05-3: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "heatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 04/Hands-on exercise 04-4.html",
    "href": "Hands-on exercise/Hands-on exercise 04/Hands-on exercise 04-4.html",
    "title": "Hands-on exercise 04-4",
    "section": "",
    "text": "pacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\n\ncovid19 &lt;- read_csv(\"COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\n\n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 04/Hands-on exercise 04-4.html#funnelplotr-methods-the-basic-plot",
    "href": "Hands-on exercise/Hands-on exercise 04/Hands-on exercise 04-4.html#funnelplotr-methods-the-basic-plot",
    "title": "Hands-on exercise 04-4",
    "section": "",
    "text": "funnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 04/Hands-on exercise 04-4.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on exercise/Hands-on exercise 04/Hands-on exercise 04-4.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on exercise 04-4",
    "section": "",
    "text": "To plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "In-class exercise/In-class exercise 05.html",
    "href": "In-class exercise/In-class exercise 05.html",
    "title": "In-class exercise 05",
    "section": "",
    "text": "Getting Started\n\npacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary)\n\nImporting Data\n\ncar_resale &lt;- read_xls(\"ToyotaCorolla.xls\",\"data\")\n\n\nsummary(car_resale)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nData Overview\n\ncar_resale %&gt;%\n  ExpData(type = 2)\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id       numeric     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month       numeric     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin     character     1436             0              0\n12    12           CC_bin     character     1436             0              0\n13    13            Doors       numeric     1436             0              0\n14    14            Gears       numeric     1436             0              0\n15    15        Cylinders       numeric     1436             0              0\n16    16        Fuel_Type     character     1436             0              0\n17    17            Color     character     1436             0              0\n18    18        Met_Color       numeric     1436             0              0\n19    19        Automatic       numeric     1436             0              0\n20    20    Mfr_Guarantee       numeric     1436             0              0\n21    21  BOVAG_Guarantee       numeric     1436             0              0\n22    22              ABS       numeric     1436             0              0\n23    23         Airbag_1       numeric     1436             0              0\n24    24         Airbag_2       numeric     1436             0              0\n25    25            Airco       numeric     1436             0              0\n26    26  Automatic_airco       numeric     1436             0              0\n27    27    Boardcomputer       numeric     1436             0              0\n28    28        CD_Player       numeric     1436             0              0\n29    29     Central_Lock       numeric     1436             0              0\n30    30  Powered_Windows       numeric     1436             0              0\n31    31   Power_Steering       numeric     1436             0              0\n32    32            Radio       numeric     1436             0              0\n33    33        Mistlamps       numeric     1436             0              0\n34    34      Sport_Model       numeric     1436             0              0\n35    35 Backseat_Divider       numeric     1436             0              0\n36    36     Metallic_Rim       numeric     1436             0              0\n37    37   Radio_cassette       numeric     1436             0              0\n38    38          Tow_Bar       numeric     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2\n\n\n\ncols &lt;- c(\"Mfg_Month\", \"HP_Bin\", \"CC_bin\", \"Doors\", \"Gears\", \"Cylinders\", \"Fuel_Type\", \"Color\", \"ABS\", \"Airbag_1\",\n          \"Met_Color\", \"Automatic\", \"Mfr_Guarantee\", \"BOVAG_Guarantee\", \"CD_Player\", \"Airbag_2\", \"Airco\", \"Automatic_airco\", \n          \"Boardcomputer\", \"Mistlamps\", \"Central_Lock\", \"Powered_Windows\", \"Power_Steering\", \"Radio\",\n          \"Sport_Model\", \"Backseat_Divider\", \"Metallic_Rim\", \"Radio_cassette\", \"Tow_Bar\")\n\n\ncar_resale &lt;- read_xls(\"ToyotaCorolla.xls\",\n                       sheet = \"data\") %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_each_(funs(factor(.)),cols)\n\n\ncar_resale %&gt;%\n  ExpNumViz(target = NULL,\n            nlim = 10,\n            Page = c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncar_resale %&gt;%\n  ExpNumViz(target = \"Price\",\n            nlim = 10,\n            Page = c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\ncar_resale %&gt;%\n  ExpCatViz(target = NULL,\n            col = \"sky blue\",\n            clim = 10,\n            margin = 2,\n            Page = c(4,4),\n            sample = 16)\n\n$`0`\n\n\n\n\n\n\n\n\n\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM +\n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n               Weight + Guarantee_Period, data = car_resale)\ncheck_normality(model1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\n\ncheck_heteroscedasticity(model1)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\nsummary(model1)\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10249.4   -768.6    -15.4    738.5   6356.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.186e+03  9.722e+02  -2.248   0.0247 *  \nAge_08_04        -1.195e+02  2.760e+00 -43.292   &lt;2e-16 ***\nKM               -2.406e-02  1.201e-03 -20.042   &lt;2e-16 ***\nWeight            1.972e+01  8.379e-01  23.533   &lt;2e-16 ***\nGuarantee_Period  2.682e+01  1.261e+01   2.126   0.0336 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1413 on 1431 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8482 \nF-statistic:  2005 on 4 and 1431 DF,  p-value: &lt; 2.2e-16\n\n\n\np_model &lt;- parameters(model1)\n\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\nlibrary(ggstatsplot)\nggcoefstats(model1,\n             output = \"plot\")"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html",
    "href": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html",
    "title": "Hands-on exercise 07",
    "section": "",
    "text": "pacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\n\nattacks &lt;- read_csv(\"eventlog.csv\")\n\n\n\n\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\n\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Deriving attack by country object\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Data Import\n\nair &lt;- read_excel(\"arrivals_by_air.xlsx\")\n\nStep 2: Deriving month and year fields\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\nStep 3: Extracting the target country\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nStep 4: Computing year average arrivals by month\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\nStep 5: Plotting the cycle plot\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Data Import\n\nrice &lt;- read_csv(\"rice.csv\")\n\nStep 2: Plotting the slopegraph\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#getting-started",
    "href": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#getting-started",
    "title": "Hands-on exercise 07",
    "section": "",
    "text": "pacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\n\nattacks &lt;- read_csv(\"eventlog.csv\")\n\n\n\n\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#data-preparation",
    "href": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#data-preparation",
    "title": "Hands-on exercise 07",
    "section": "",
    "text": "Step 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#building-the-calendar-heatmaps",
    "href": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#building-the-calendar-heatmaps",
    "title": "Hands-on exercise 07",
    "section": "",
    "text": "grouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#plotting-multiple-calendar-heatmaps",
    "href": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#plotting-multiple-calendar-heatmaps",
    "title": "Hands-on exercise 07",
    "section": "",
    "text": "Step 1: Deriving attack by country object\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#plotting-cycle-plot",
    "href": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#plotting-cycle-plot",
    "title": "Hands-on exercise 07",
    "section": "",
    "text": "Step 1: Data Import\n\nair &lt;- read_excel(\"arrivals_by_air.xlsx\")\n\nStep 2: Deriving month and year fields\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\nStep 3: Extracting the target country\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nStep 4: Computing year average arrivals by month\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\nStep 5: Plotting the cycle plot\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#plotting-slopegraph",
    "href": "Hands-on exercise/Hands-on exercise 07/Hands-on exercise 07.html#plotting-slopegraph",
    "title": "Hands-on exercise 07",
    "section": "",
    "text": "Step 1: Data Import\n\nrice &lt;- read_csv(\"rice.csv\")\n\nStep 2: Plotting the slopegraph\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]